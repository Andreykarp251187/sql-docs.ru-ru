---
title: Настройка кластера SLES для группы доступности SQL Server
titleSuffix: SQL Server
description: Сведения о создании кластеров группы доступности для SQL Server в SUSE Linux Enterprise Server (SLES)
author: MikeRayMSFT
ms.author: mikeray
ms.reviewer: vanto
ms.date: 04/30/2018
ms.topic: conceptual
ms.prod: sql
ms.technology: linux
ms.assetid: 85180155-6726-4f42-ba57-200bf1e15f4d
ms.openlocfilehash: 063adf4f1f180138150484e4ac9fc397ef886f5d
ms.sourcegitcommit: b2464064c0566590e486a3aafae6d67ce2645cef
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/15/2019
ms.locfileid: "68003557"
---
# <a name="configure-sles-cluster-for-sql-server-availability-group"></a>Настройка кластера SLES для группы доступности SQL Server

[!INCLUDE[appliesto-ss-xxxx-xxxx-xxx-md-linuxonly](../includes/appliesto-ss-xxxx-xxxx-xxx-md-linuxonly.md)]

Это руководство содержит инструкции по созданию кластер с тремя узлами для SQL Server в SUSE Linux Enterprise Server (SLES) 12 с пакетом обновления 2. Для обеспечения высокой доступности группы доступности в Linux требует трех узлов — см. в разделе [высокий уровень доступности и защиты данных для конфигураций группы доступности](sql-server-linux-availability-group-ha.md). Кластеризации уровень основан на SUSE [высокий уровень доступности расширения (для которых Имеется)](https://www.suse.com/products/highavailability) создаются на основе [Pacemaker](https://clusterlabs.org/). 

Дополнительные сведения о конфигурации кластера, параметры агента ресурсов, управления, рекомендации и рекомендации, см. в разделе [SUSE Linux Enterprise высокого уровня доступности расширения 12 SP2](https://www.suse.com/documentation/sle-ha-12/index.html).

>[!NOTE]
>На этом этапе не является образом связанных с WSFC в Windows, как интеграция с SQL Server с помощью Pacemaker в Linux. Службы SQL Server в Linux не поддерживает кластеры. Pacemaker контролирует все оркестрации ресурсов кластера, включая ресурс группы доступности. В Linux не следует полагаться на всегда на доступности группы динамические административные представления (DMV), предоставляющие информацию о кластере, например sys.dm_hadr_cluster. Кроме того относится только к WSFC имя виртуальной сети, не имеет эквивалента в одной и той же в Pacemaker. По-прежнему можно создать прослушиватель, чтобы использовать его для прозрачного переподключения после отработки отказа, но необходимо вручную зарегистрировать имя прослушивателя в DNS-сервер с IP-адрес, используемый для создания виртуальный IP-адрес (как описано в следующих разделах).


## <a name="roadmap"></a>Стратегия развития

Процедура создания группы доступности для обеспечения высокой доступности отличается для серверов Linux и отказоустойчивый кластер Windows Server. Ниже перечислены основные действия. 

1. [Настройка SQL Server на узлах кластера](sql-server-linux-setup.md).

2. [Создание группы доступности](sql-server-linux-availability-group-failover-ha.md). 

3. Настройте диспетчер кластерных ресурсов, таких как Pacemaker. Эти инструкции приведены в этом документе.
   
   Способ настройки диспетчер ресурсов кластера зависит от конкретного дистрибутива Linux. 

   >[!IMPORTANT]
   >Рабочих средах требуется агент ограждения, например STONITH для обеспечения высокой доступности. Примеры в этой статье не используют агентов ограждения. Они предназначены для тестирования и проверки только. 
   
   >Кластер Pacemaker использует ограждения для возврата кластера в известное состояние. Способ настройки ограждения зависит от распределения и среды. В настоящее время ограждения недоступна в некоторых облачных средах. См. в разделе [расширение высокого уровня доступности SUSE Linux Enterprise](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.fencing).

5. [Добавить в группу доступности в качестве ресурса кластера](sql-server-linux-availability-group-cluster-sles.md#configure-the-cluster-resources-for-sql-server). 

## <a name="prerequisites"></a>предварительные требования

Для работы в следующем сценарии end-to-end, требуется три машины для развертывания трех узлов кластера. Ниже приведены инструкции для настройки этих серверов.

## <a name="setup-and-configure-the-operating-system-on-each-cluster-node"></a>Установка и настройка операционной системы на каждом узле кластера 

Первым шагом является настройка операционной системы на узлах кластера. Для в этом пошаговом руководстве используйте SLES 12 SP2 с действительной подпиской для надстройки высокого уровня ДОСТУПНОСТИ.

### <a name="install-and-configure-sql-server-service-on-each-cluster-node"></a>Установка и настройка службы SQL Server на каждом узле кластера

1. Установка и настройка службы SQL Server на всех узлах. Подробные инструкции см. в разделе [Установка SQL Server в Linux](sql-server-linux-setup.md).

1. Назначить один узел в качестве основного и другие узлы в качестве базы данных-получатели. Использовать эти термины в этом руководстве.

1. Убедитесь, что узлы, которые будут частью кластера могут взаимодействовать друг с другом.

   В следующем примере показан `/etc/hosts` с дополнениями для трех узлов с именем SLES1 SLES2 и SLES3.

   ```
   127.0.0.1   localhost
   10.128.16.33 SLES1
   10.128.16.77 SLES2
   10.128.16.22 SLES3
   ```

   Все узлы кластера должны иметь возможность обращаться друг к другу через SSH. Средства, такие как `hb_report` или `crm_report` (для устранения неполадок) и обозреватель журнала Hawk требуют доступа SSH между узлами, в противном случае они могут только собирать данные из текущего узла. Если вы используете нестандартный порт SSH, используйте параметр -X (см. в разделе `man` страницы). Например, если SSH-порт 3479, вызовите `crm_report` с:

   ```bash
   sudo crm_report -X "-p 3479" [...]
   ```

   Дополнительные сведения см. в разделе [руководство по администрированию SLES — раздел "Прочее"](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.troubleshooting.misc).


## <a name="create-a-sql-server-login-for-pacemaker"></a>Создание имени входа SQL Server для Pacemaker

[!INCLUDE [SLES-Create-SQL-Login](../includes/ss-linux-cluster-pacemaker-create-login.md)]

## <a name="configure-an-always-on-availability-group"></a>Настройка группы доступности Always On

На серверах Linux настройте группы доступности, а затем настройте кластерные ресурсы. Чтобы настроить группы доступности, см. в разделе [Настройка группы доступности AlwaysOn для SQL Server в Linux](sql-server-linux-availability-group-configure-ha.md)

## <a name="install-and-configure-pacemaker-on-each-cluster-node"></a>Установка и настройка Pacemaker на каждом узле кластера

1. Установите расширение высокого уровня доступности

   Справочные материалы по [установке SUSE Linux Enterprise Server и высокий уровень доступности расширения](https://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.installation)

1. Установите пакет агента ресурсов SQL Server на обоих узлах.

   ```bash
   sudo zypper install mssql-server-ha
   ```

## <a name="set-up-the-first-node"></a>Настройка первого узла

   Ссылаться на [инструкции по установке SLES](https://www.suse.com/documentation/sle-ha-12/singlehtml/install-quick/install-quick.html#sec.ha.inst.quick.setup.1st-node)

1. Войдите в систему как `root` физической или виртуальной машины, которые вы хотите использовать в качестве узла кластера.
2. Запустите скрипт начальной загрузки, выполнив:
   ```bash
   sudo ha-cluster-init
   ```

   Если не был настроен NTP для включения во время загрузки, появится сообщение. 

   Если вы решите продолжить, сценарий автоматически создает ключи для доступа по протоколу SSH, а также для средства синхронизации со Csync2 и запуск служб, необходимых для обоих. 

3. Настройка уровня связи кластеров (Corosync): 

   1\. Введите адрес для доступа к сети. По умолчанию он предлагает сетевой адрес eth0. Кроме того введите отдельный сетевой адрес, например, адрес bond0. 

   2\. Введите адрес многоадресной рассылки. Он предлагает случайно выбранный адрес, который можно использовать по умолчанию. 

   В. Введите порт многоадресной рассылки. Он предлагает 5405 по умолчанию. 

   Г. Чтобы настроить `SBD ()`, введите постоянного пути к разделу блок устройства, которое вы хотите использовать для одновременной передачи данных. Пути должны быть согласованы по всем узлам в кластере. 
   Наконец сценарий запускает службу Pacemaker для одного узла кластера и включение веб-интерфейс управления Hawk2. URL-адрес, используемый для Hawk2 отображается на экране. 

4. Все сведения о процессе установки см. `/var/log/sleha-bootstrap.log`. Теперь у вас есть кластер работает одним узлом. Проверьте состояние кластера с состоянием crm:

   ```bash
   sudo crm status
   ```

   Также вы увидите конфигурации кластера с `crm configure show xml` или `crm configure show`.

5. Процедура начальной загрузки создается пользователь Linux hacluster, с помощью пароль linux. Как можно скорее замените более безопасный пароль по умолчанию: 

   ```bash
   sudo passwd hacluster
   ```

## <a name="add-nodes-to-the-existing-cluster"></a>Добавить узлы в существующий кластер

Если у вас есть кластер, работающий с одного или нескольких узлов, добавьте дополнительные узлы кластера с помощью начальной загрузки сценария ha-cluster-join. Сценарий требуется только доступ к существующим узлом кластера и автоматически завершать базовую настройку на текущем компьютере. Выполните следующие действия:

Если вы настроили для существующих узлов кластера с `YaST` кластера модуля, убедитесь, что будут выполнены следующие необходимые компоненты перед запуском `ha-cluster-join`:
- Привилегированному пользователю на существующие узлы предусмотрена ключи SSH для входа без пароля. 
- `Csync2` настраивается на существующие узлы. Дополнительные сведения см. в разделе Настройка Csync2 с помощью YaST. 

1. Войдите в качестве корневой для физической или виртуальной машины должен соединиться с кластером. 
2. Запустите скрипт начальной загрузки, выполнив: 

   ```bash
   sudo ha-cluster-join
   ```

   Если не был настроен NTP для включения во время загрузки, появится сообщение. 

3. Если вы решите продолжить, появится для IP-адреса существующим узлом. Введите IP-адрес. 

4. Если вы еще не настроили доступа SSH между обоих компьютерах, также появится для существующего узла пароль учетной записи root. 

   Войдя на указанный узел, сценарий копирует конфигурацию Corosync, настраивает SSH и `Csync2`и переводит текущего компьютера через Интернет как новый узел кластера. Кроме того, она запускает службу, необходимые для Hawk. Если вы настроили общее хранилище с `OCFS2`, он также автоматически создает каталог точку подключения для `OCFS2` файловой системы. 

5. Повторите предыдущие шаги для всех компьютеров, которые вы хотите добавить в кластер. 

6. Сведения о процессе проверки `/var/log/ha-cluster-bootstrap.log`. 

1. Проверьте состояние кластера с помощью `sudo crm status`. Если второй узел добавлен успешно, выходные данные должны быть примерно следующими:

   ```bash
   sudo crm status
   
   3 nodes configured
   1 resource configured
   Online: [ SLES1 SLES2 SLES3]
   Full list of resources:   
   admin_addr     (ocf::heartbeat:IPaddr2):       Started node1
   ```

   >[!NOTE]
   >`admin_addr` — Это виртуальный IP-адрес кластера, настроенный во время установки исходного кластера с одним узлом.

После добавления всех узлов, проверьте, если необходимо настроить нет кворума политику в параметрах глобального кластера. Это особенно важно для кластеров с двумя узлами. Дополнительные сведения см. разделе 4.1.2, параметр нет кворума policy. 

## <a name="set-cluster-property-cluster-recheck-interval"></a>Установка свойства кластера кластера перепроверка интервала

`cluster-recheck-interval` Указывает интервал опроса, по которому проверяет кластера, для изменения параметров ресурсов, ограничения или другие параметры кластера. Если реплика выходит из строя, кластер пытается выполнить перезапуск реплики с интервалом, привязанного с `failure-timeout` значение и `cluster-recheck-interval` значение. Например если `failure-timeout` составляет 60 секунд и `cluster-recheck-interval` имеет значение 120 секунд, производится попытка перезагрузки с интервалом, превышает 60 секунд, но менее 120 секунд. Рекомендуется задать время ожидания сбоя 60s и кластера перепроверка-значение интервала больше 60 секунд. Установка кластера перепроверка интервала небольшое значение не рекомендуется.

Чтобы обновить значение свойства для `2 minutes` запуска:

```bash
crm configure property cluster-recheck-interval=2min
```

> [!IMPORTANT] 
> Если вас уже есть ресурс группы доступности, под управлением кластера Pacemaker, обратите внимание, что все распределения, использующих последние доступные Pacemaker пакет 1.1.18-11.el7 вносит изменение поведения для начала сбоя — Неустранимая кластера, параметр, если его значение — false. Это изменение затрагивает рабочий процесс отработки отказа. Если первичная реплика, произошел сбой, ожидается кластера отработки отказа в одной из доступных вторичных реплик. Вместо этого пользователи заметят, что кластер сисадмин пытается запустить сбой первичной реплики. Если этой основной никогда не подключается к сети (из-за постоянных сбой), никогда не перехода кластера на другой доступную вторичную реплику. Из-за этого изменения рекомендованные конфигурации для задания начала сбоя — Неустранимая больше не является допустимым и должен вернуть значение по умолчанию `true`. Кроме того, ресурс группы Доступности должен быть обновлен для включения `failover-timeout` свойство. 
>
>Чтобы обновить значение свойства для `true` запуска:
>
>```bash
>crm configure property start-failure-is-fatal=true
>```
>
>Обновить существующие свойства ресурсов группы Доступности `failure-timeout` для `60s` запуска (Замените `ag1` с именем ресурса группы доступности): 
>
>```bash
>crm configure edit ag1
># In the text editor, add `meta failure-timeout=60s` after any `param`s and before any `op`s
>```

Дополнительные сведения о свойствах кластера Pacemaker, см. в разделе [Настройка ресурсов кластера](https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_config_crm_resources.html).

## <a name="configure-fencing-stonith"></a>Настройка ограждения (STONITH)
Поставщики кластера pacemaker требуются STONITH включения и ограждения устройства, настроенные для настройки кластера, поддерживаемых. Если диспетчер кластерных ресурсов не может определить состояние узла или ресурса на узле, ограждения используется для ввода кластера в известное состояние еще раз.

Ресурс уровня ограждения главным образом, гарантируется без повреждения данных во время сбоя, настроив ресурса. Можно использовать ограждения уровня ресурсов, например, с DRBD (Distributed реплицированные блочное устройство) для обозначения диска на узле как устаревшие, когда канал связи выходит из строя.

Узел уровня ограждения гарантирует, что узел не все ресурсы. Это делается путем сброса параметров узла и его реализация Pacemaker вызывается STONITH (что означает «устранение неисправностей другой узел в заголовке»). Pacemaker поддерживает разнообразные полезные ограждения устройств, таких как источник бесперебойного питания питания или управления картами для серверов.

Дополнительные сведения см. в разделе [кластеров Pacemaker с нуля](https://clusterlabs.org/pacemaker/doc/en-US/Pacemaker/1.1/html/Clusters_from_Scratch/), [ограждения и Stonith](https://clusterlabs.org/doc/crm_fencing.html) и [SUSE HA документации: Ограждения и STONITH](https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_fencing.html).

Во время инициализации кластера STONITH будет отключен, если конфигурация не обнаруживается. Его можно включить позже, выполнив следующую команду:

```bash
sudo crm configure property stonith-enabled=true
```
  
>[!IMPORTANT]
>Отключение STONITH — только для целей тестирования. Если вы планируете использовать Pacemaker в рабочей среде, следует планировать реализацию STONITH в зависимости от среды и оставьте флажок. SUSE не поддерживает агентов ограждения для облачных сред (в том числе Azure) или Hyper-V. Следовательно кластера поставщика не обеспечивает поддержку для запуска рабочих кластеров в этих средах. Мы работаем над решением этот пробел, которая будет доступна в будущих выпусках.


## <a name="configure-the-cluster-resources-for-sql-server"></a>Настройте кластерные ресурсы для SQL Server

Ссылаться на [SLES администрирования Guid](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.manual_config)

## <a name="enable-pacemaker"></a>Включить Pacemaker

Включите Pacemaker, таким образом, чтобы он запускается автоматически.

Выполните следующую команду на каждом узле в кластере.

```bash
systemctl enable pacemaker
```

### <a name="create-availability-group-resource"></a>Создайте ресурс группы доступности

Следующая команда создает и настраивает ресурс группы доступности для трех реплик группы доступности [ag1]. Мониторинг операций и время ожидания должно быть задано явным образом в SLES основана на том, что время ожидания зависят от высокой рабочей нагрузки и необходимо тщательно настроить пороговое значение для каждого развертывания.
Выполните команду на одном из узлов в кластере:

1. Запустите `crm configure` чтобы открыть строку crm:

   ```bash
   sudo crm configure 
   ```

1. В этой строке crm выполните следующую команду, чтобы настроить свойства ресурса.

   ```bash
   primitive ag_cluster \
      ocf:mssql:ag \
      params ag_name="ag1" \
      meta failure-timeout=60s \
      op start timeout=60s \
      op stop timeout=60s \
      op promote timeout=60s \
      op demote timeout=10s \
      op monitor timeout=60s interval=10s \
      op monitor timeout=60s interval=11s role="Master" \
      op monitor timeout=60s interval=12s role="Slave" \
      op notify timeout=60s
   ms ms-ag_cluster ag_cluster \
      meta master-max="1" master-node-max="1" clone-max="3" \
     clone-node-max="1" notify="true" \
   commit
      ```

[!INCLUDE [required-synchronized-secondaries-default](../includes/ss-linux-cluster-required-synchronized-secondaries-default.md)]

### <a name="create-virtual-ip-resource"></a>Создайте виртуальный IP-адрес

Если вы не создали виртуальный IP-адрес при запуске `ha-cluster-init` теперь можно создать этот ресурс. Следующая команда создает виртуальный IP-адрес. Замените `<**0.0.0.0**>` с доступный адрес из вашей сети и `<**24**>` с количество битов маски подсети CIDR. Запустите на каждом узле.

```bash
crm configure \
primitive admin_addr \
   ocf:heartbeat:IPaddr2 \
   params ip=<**0.0.0.0**> \
      cidr_netmask=<**24**>
```

### <a name="add-colocation-constraint"></a>Добавить ограничение среды для совместной работы
Практически каждое решение в кластере Pacemaker, такие как выбор, где должны запускаться ресурс, выполняется путем сравнения оценок. Вычисления оценки для одного ресурса, и диспетчер кластерных ресурсов выбирает узел с наивысшей оценкой для конкретного ресурса. (Если на узле имеется отрицательной оценки для ресурса, ресурс нельзя запустить на этом узле.) Мы можем оперировать решения кластера с ограничениями. Ограничения имеют оценку. Если ограничение имеет показатель ниже, чем БЕСКОНЕЧНОСТИ, это только рекомендация. Оценка бесконечность означает, что он является обязательным. Мы хотим первичной группы доступности и виртуальный IP-ресурс следует запускать на одном узле, поэтому мы определим ограничение совместного размещения с оценкой бесконечность. 

Чтобы задать ограничение совместного размещения для виртуального IP-адреса для запуска на том же узле, master, выполните следующую команду на одном узле:

```bash
crm configure
colocation vip_on_master inf: \
    admin_addr ms-ag_cluster:Master
commit
```

### <a name="add-ordering-constraint"></a>Добавление ограничения упорядочивания
Ограничения совместного размещения имеет неявное ограничение упорядочивания. Он перемещает виртуальный IP-адрес до перемещения ресурса группы доступности. По умолчанию является последовательность событий: 

1. Ресурс пользователя проблемы миграции к хозяину группы доступности с узла 1 на узел node2.
2. Виртуальный IP-адрес останавливается на узле 1.
3. Виртуальный IP-адрес запускается на узле 2. На этом этапе IP-адрес временно точки к узлу 2 а узел 2 — по прежнему до отработки отказа вторичной. 
4. Основной группы доступности на узле 1, переводится в режим ведомый.
5. Подчиненный узел группы доступности на узле 2 повышается до главного. 

Чтобы предотвратить временно, указывающие на узел с дополнительным до отработки отказа IP-адрес, добавьте ограничения упорядочивания. Чтобы добавить ограничения упорядочивания, выполните следующую команду на одном узле: 

```bash
crm crm configure \
   order ag_first inf: ms-ag_cluster:promote admin_addr:start
```


>[!IMPORTANT]
>После настройки кластера и добавлению группы доступности в качестве ресурса кластера, Transact-SQL нельзя использовать для отработки отказа ресурсов этой группы доступности. Ресурсы кластера SQL Server в Linux не связаны тесно как с операционной системой, как они находятся на кластер отработки отказа Windows Server (WSFC). Службы SQL Server не учитывает наличие кластера. Все согласование осуществляется с помощью средства управления кластером. В SLES использовать `crm`. 

Вручную переключить группу доступности с `crm`. Не инициировать отработку отказа с помощью Transact-SQL. Дополнительные сведения см. в разделе [отработки отказа](sql-server-linux-availability-group-failover-ha.md#failover).


Дополнительные сведения см. в разделе:
- [Управление кластерными ресурсами](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#sec.ha.config.crm).   
- [Высокая ДОСТУПНОСТЬ основные понятия](https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#cha.ha.concepts)
- [Краткий справочник pacemaker](https://github.com/ClusterLabs/pacemaker/blob/master/doc/pcs-crmsh-quick-ref.md) 

<!---[!INCLUDE [Pacemaker Concepts](..\includes\ss-linux-cluster-pacemaker-concepts.md)]--->

## <a name="next-steps"></a>Следующие шаги

[Работать с высоким уровнем ДОСТУПНОСТИ группы доступности](sql-server-linux-availability-group-failover-ha.md)
